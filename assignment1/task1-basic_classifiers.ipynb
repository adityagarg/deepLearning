{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECBM4040: Assignment 1, Task 1 - Basic ML Classifiers\n",
    "\n",
    "In this task, you are going to implement two classifers and apply them to the  CIFAR-10 dataset: \n",
    "\n",
    "(1) Linear SVM classifier, and\n",
    "\n",
    "(2) Softmax classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from ecbm4040.cifar_utils import load_data\n",
    "\n",
    "# Plot configurations\n",
    "% matplotlib inline\n",
    "\n",
    "# Notebook auto reloads code. (Ref: http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython)\n",
    "% load_ext autoreload\n",
    "% autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CIFAR-10 data\n",
    "\n",
    "CIFAR-10 is a widely used dataset which contains 60,000 color images of size 32x32 divided into 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images. Here you are going to use a small part (~500) of data (images) for classification.\n",
    "\n",
    "See https://www.cs.toronto.edu/~kriz/cifar.html. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (50000, 3072)\n",
      "Training labels shape:  (50000,)\n",
      "Test data shape:  (10000, 3072)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the raw CIFAR-10 data.\n",
    "X_train, y_train, X_test, y_test = load_data()\n",
    "\n",
    "# We have vectorized the data (rearranged the storage of images) for you. \n",
    "# That is, we flattened 32×32×3 images into 1×3072 Numpy arrays. Number 3 stands for 3 colors.\n",
    "# The reason we do this is because we can not put 3-D image representations into our model. \n",
    "# This is common practice (flattening images before putting them into the ML model). \n",
    "# Note that this practice may not be used for Convolutional Neural Networks (CNN). \n",
    "# We will later see how we manage the data when used in CNNs.\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3072)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3072)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3072)\n",
      "Test labels shape:  (1000,)\n",
      "Development data shape: (100, 3072)\n",
      "Development data shape (100,)\n"
     ]
    }
   ],
   "source": [
    "# Data organization:\n",
    "#    Train data: 49,000 samples from the original train set: indices 1~49,000\n",
    "#    Validation data: 1,000 samples from the original train set: indices 49,000~50,000\n",
    "#    Test data: 1,000 samples from the original test set: indices 1~1,000\n",
    "#    Development data (for gradient check): 100 samples from the train set: indices 1~49,000\n",
    "num_training = 49000\n",
    "num_validation = 1000\n",
    "num_test = 1000\n",
    "num_dev = 100\n",
    "\n",
    "X_val = X_train[-num_validation:, :]\n",
    "y_val = y_train[-num_validation:]\n",
    "\n",
    "mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "X_dev = X_train[mask]\n",
    "y_dev = y_train[mask]\n",
    "\n",
    "X_train = X_train[:num_training, :]\n",
    "y_train = y_train[:num_training]\n",
    "\n",
    "X_test = X_test[:num_test, :]\n",
    "y_test = y_test[:num_test]\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('Development data shape:', X_dev.shape)\n",
    "print('Development data shape', y_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49000, 3073) (1000, 3073) (1000, 3073) (100, 3073)\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing: subtract the mean value across every dimension for training data\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "\n",
    "X_train = X_train.astype(np.float32) - mean_image.astype(np.float32)\n",
    "X_val = X_val.astype(np.float32) - mean_image\n",
    "X_test = X_test.astype(np.float32) - mean_image\n",
    "X_dev = X_dev.astype(np.float32) - mean_image\n",
    "\n",
    "# Append the bias dimension of ones (i.e. bias trick) so that our SVM\n",
    "# only has to worry about optimizing a single weight matrix W.\n",
    "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape, X_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Linear Support Vector Machine (SVM) Classifier\n",
    "\n",
    "In this part, you are going to implement a linear SVM classifier. \n",
    "\n",
    "Excellent summaries of SVM methods are presented in ([John Paisley, \"Machine Learning for Data Science,\" SVM_slides](http://www.columbia.edu/~jwp2128/Teaching/W4721/Spring2017/slides/lecture_2-23-17.pdf)) and ([David Sontag, \"Introduction to Machine Learning,\", New York University, Lectures 4,5,6 ](http://people.csail.mit.edu/dsontag/courses/ml12/slides/lecture4.pdf)).\n",
    "\n",
    "<span style=\"color:red\"><strong>TODO</strong></span>: The code is given in **./ecbm4040/classifiers/linear_svm.py**, where a naive linear SVM is provided. You need to understand the algorithm and implement a linear SVM using fully vectorized operations with numpy. When you finish the code, run the block below - it will help you verify your result. The loss error should be around 1e-14, while the gradient error should be below 1e-11. Moreover, the vectorized SVM should be much faster than the naive SVM.\n",
    "\n",
    "**Brief introduction to SVM**\n",
    "\n",
    "Support Vector Mahcine(SVM) is a very important supervised classification model developed in 1992. ([Original paper by Boser et al.](http://dl.acm.org/citation.cfm?id=130401)). It can be used not only for binary classification problems, but also for multiclass classification. As our course reference books points out:\n",
    "> One key innovation associated with support vector machines is the _kernel tricks_.\n",
    "\n",
    "SVM is a __max margin classifier__ that tries to maximize the __margin__ between clusters of data points. The __margin__ between a boundary hyperplane and a cluster is defined as the minimal distance from the points inside the cluster to the boundary. Intuitively speaking, the classification boundary should be as far away from any cluster as possible. \n",
    "\n",
    "![classifier_graph](./img/SVM1.png)\n",
    "\n",
    "The picture above shows what a SVM boundary could look like, in a 2-D plane. Notice that in the left image, the boundary is good enough to distinguish the 2 clusters, but the margins are small (at least one point from a cluster is very close to the boundary). In the image on the right, the boundary separates the 2 clusters, and it is also far from each of the clusters - this is a good SVM boundary. (Image source: Prof. John Paisley, ([_Machine Learning for Data Science_](http://www.columbia.edu/~jwp2128/Teaching/W4721/Spring2017/W4721Spring2017.html)), Columbia University, spring 2017. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Here we discuss the multi-class linear SVM**\n",
    "\n",
    "The prediction model of a linear SVM is:\n",
    "\n",
    "$$\\mathbf{y}_{prediction} = argmax(\\mathbf{x}\\times \\mathbf{W} + \\mathbf{b})$$\n",
    "\n",
    "The $argmax$ function outputs the index of the maximum element for each $x_i$.\n",
    "\n",
    "To train this SVM model, we usually consider the following optimization problem:\n",
    "\n",
    "$$\\min_{\\mathbf{W}} \\frac{1}{2}||\\mathbf{W}||^2 $$\n",
    "<!-- ## $$\\min_{W} \\quad \\frac{1}{2}||W||^2$$ -->\n",
    "\n",
    "such that $$\\quad \\mathbf{x}_i \\times \\mathbf{W}_{\\mathbf{y}_i}-\\mathbf{x}_i \\times \\mathbf{W}_c\\geq \\delta,\\quad for \\ i=1,...N, \\quad c=1,...,C,$$\n",
    "\n",
    "<!-- $$such \\quad that \\quad \\mathbf{x_i \\times W_{y_i}-x_i \\times W_c\\geq \\delta},\\quad for \\ i=1,...N, \\quad c=1,...,C$$  -->\n",
    "\n",
    "where $N$ is the total number of training samples while $C$ is the number of classes.\n",
    "\n",
    "Here we do not consider the bias term because we have already expanded the bias dimension with all ones in input $\\mathbf{x}$. The value of $\\delta$ indicates the width of the margin (there are no training points within the width of the margin, from any cluster). To make a more restricted classification, we could choose a larger $\\delta$. However, for a more robust classification (sometimes raw data contains a lot of noise, and the separable plane cannot be found), $\\delta$ can have a negative value. In that case, we need to add a penalty term wrt $\\delta$ into the objective function. Here the default $\\delta$ is 1.\n",
    "\n",
    "To solve the problem above, one needs to convert it into its Lagrangian dual form. We do not cover the details here. The final loss function is:\n",
    "\n",
    "$$L=\\alpha\\times \\frac{1}{2}\\times||\\mathbf{W}||^2+\\sum_i\\sum_c \\max(0,\\delta + \\mathbf{x}_i \\times \\mathbf{W}_c - \\mathbf{x}_i \\times \\mathbf{W}_{y_i})$$\n",
    "\n",
    "To determine the bondary hyperplane, we need to compute $\\mathbf{W}$, which will be obtained by minimizing $L$ over all trainable variables. The $\\alpha$ is a hyperparameter which we set for penalizing the l2-norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sv_loss_naive(W, X, y, reg):\n",
    "    \"\"\"\n",
    "    Multi-class Linear SVM loss function, naive implementation (with loops).\n",
    "    \n",
    "    In default, delta is 1 and there is no penalty term wst delta in objective function.\n",
    "\n",
    "    Inputs have dimension D, there are C classes, and we operate on minibatches\n",
    "    of N examples.\n",
    "\n",
    "    Inputs:\n",
    "    - W: a numpy array of shape (D, C) containing weights.\n",
    "    - X: a numpy array of shape (N, D) containing N samples.\n",
    "    - y: a numpy array of shape (N,) containing training labels; y[i] = c means\n",
    "         that X[i] has label c, where 0 <= c < C.\n",
    "    - reg: (float) L2 regularization strength\n",
    "\n",
    "    Returns:\n",
    "    - loss: a float scalar\n",
    "    - gradient: wrt weights W, an array of same shape as W\n",
    "    \"\"\"\n",
    "    dW = np.zeros(W.shape).astype('float') # initialize the gradient as zero\n",
    "\n",
    "    # compute the loss and the gradient\n",
    "    num_classes = W.shape[1]\n",
    "    num_train = X.shape[0]\n",
    "    loss = 0.0\n",
    "    for i in range(num_train):\n",
    "        scores = X[i].dot(W)\n",
    "        correct_class_score = scores[y[i]]\n",
    "        for j in range(num_classes):\n",
    "            if j == y[i]:\n",
    "                continue\n",
    "            margin = scores[j] - correct_class_score + 1 # note delta = 1\n",
    "            if margin > 0:\n",
    "                loss += margin\n",
    "                dW[:,j] += X[i]\n",
    "                dW[:,y[i]] -= X[i]\n",
    "\n",
    "    # Right now the loss is a sum over all training examples, but we want it\n",
    "    # to be an average instead so we divide by num_train.\n",
    "    loss /= num_train\n",
    "    dW /= num_train\n",
    "    # Add regularization to the loss.\n",
    "    loss += reg * np.sum(W * W)\n",
    "    dW += reg*2*W\n",
    "\n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-28-1e6bbc67fca6>, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-28-1e6bbc67fca6>\"\u001b[0;36m, line \u001b[0;32m18\u001b[0m\n\u001b[0;31m    print(\"vector scores-\" scores.shape)\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def sv_loss_vectorized(W, X, y, reg):\n",
    "    \"\"\"\n",
    "    Linear SVM loss function, vectorized implementation.\n",
    "    Inputs and outputs are the same as svm_loss_naive.\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    dW = np.zeros(W.shape).astype('float') # initialize the gradient as zero\n",
    "\n",
    "\n",
    "    #############################################################################\n",
    "    # TODO:                                                                     #\n",
    "    # Implement a vectorized version of the structured SVM loss, storing the    #\n",
    "    # result in loss.                                                           #\n",
    "    #############################################################################\n",
    "    scores=np.dot(X, W)\n",
    "    correct_class_score=scores[y]\n",
    "    margin=scores - correct_class_score + 1\n",
    "    print(\"vector scores-\" scores.shape)\n",
    "    print(\"vector cClassScore-\" correct_class_score.shape)\n",
    "\n",
    "\n",
    "    \n",
    "    #############################################################################\n",
    "    # TODO:                                                                     #\n",
    "    # Implement a vectorized version of the gradient for the structured SVM     #\n",
    "    # loss, storing the result in dW.                                           #\n",
    "    #                                                                           #\n",
    "    # Hint: Instead of computing the gradient from scratch, it may be easier    #\n",
    "    # to reuse some of the intermediate values that you used to compute the     #\n",
    "    # loss.                                                                     #\n",
    "    #############################################################################\n",
    "\n",
    "    \n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "\n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive numpy loss: 9.105233970233563, takes 0.05902290344238281 seconds.\n",
      "(100, 10)\n",
      "(100, 10)\n",
      "vectorized numpy loss: 0.0, takes 0.0024521350860595703 seconds.\n",
      "****************************************************************************************************\n",
      "Relative loss error is 9.105233970233563\n",
      "Relative gradient error is 3997.6587839907074\n",
      "Is vectorized loss correct? False\n",
      "Is vectorized gradient correct? False\n"
     ]
    }
   ],
   "source": [
    "from ecbm4040.classifiers.linear_svm import svm_loss_naive\n",
    "# from ecbm4040.classifiers.linear_svm import svm_loss_vectorized\n",
    "\n",
    "# generate a random SVM weight matrix seeded with small numbers\n",
    "np.random.seed(2321)\n",
    "W = np.random.randn(3073, 10) * 0.0001 \n",
    "\n",
    "# naive numpy implementation of SVM\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = sv_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive numpy loss: {}, takes {} seconds.'.format(loss_naive, toc-tic))\n",
    "\n",
    "# vectorized numpy implementation of SVM\n",
    "tic = time.time()\n",
    "loss_vec, grad_vec = sv_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized numpy loss: {}, takes {} seconds.'.format(loss_vec, toc-tic))\n",
    "\n",
    "# check the correctness\n",
    "print('*'*100)\n",
    "print('Relative loss error is {}'.format(abs(loss_vec-loss_naive)))\n",
    "grad_err = np.linalg.norm(grad_naive - grad_vec, ord='fro')\n",
    "print('Relative gradient error is {}'.format(grad_err))\n",
    "print('Is vectorized loss correct? {}'.format(np.allclose(loss_naive, loss_vec)))\n",
    "print('Is vectorized gradient correct? {}'.format(np.allclose(grad_naive, grad_vec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 2: Softmax classifier\n",
    "\n",
    "Softmax function is a normalized exponential function. Suppose the input is a $K$-dimensional vector $\\mathbf{z}$. The softmax function is given by\n",
    "\n",
    "$$\\sigma({\\rm \\mathbf{z}})_j = \\frac{e^{z_j}}{\\sum_{k=1}^K e^{z_k}}, for\\  j=1,...,K.$$\n",
    "\n",
    "In probability theory, softmax can be explained as a categorical distribution, and that is why it is always used in multiclass classification problem. Hence, this is an important function and you will usually use it in the future deep learning experiments. Here you are going to implement a basic softmax layer. Suppose the input $\\mathbf{X}$ with shape $N \\times D$ and a corresponding label vector $\\mathbf{y}$ with length $N$, where $N$ is the number of samples and $D$ is the number of features. The loss of the softmax output is given by\n",
    "\n",
    "$$loss = \\sum_{i=1}^N -{\\rm log}\\left( softmax(\\mathbf{W} \\times X_i)_{y_i} \\right)$$\n",
    "\n",
    "In most cases, you also need to consider a bias term $b$ with length D. However, in this experiment, since a bias dimension has been added into the $X$, you can ignore it. \n",
    "\n",
    "<span style=\"color:red\"><strong>TODO</strong></span>: Finish the code in **./ecbm4040/classifiers/softmax.py** and you should implement a softmax layer in three ways. For the first two implementations, we provide the check code for you to verify your code. \n",
    "\n",
    "* Naive method using for-loop\n",
    "* Vectorized method\n",
    "* Softmax in Tensorflow. This step will familiarize you with tensorflow functions. You can refer to the check code.\n",
    "\n",
    "Do not forget the $L2$ regularization term in the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4)\n",
      "(3,)\n",
      "[ 3  8 12]\n"
     ]
    }
   ],
   "source": [
    "a=np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12]])\n",
    "print(a.shape)\n",
    "b=np.array([2,3,3])\n",
    "print(b.shape)\n",
    "blah=a[range(a.shape[0]), b]\n",
    "print(blah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive numpy loss: 0.0, takes 0.00011515617370605469 seconds.\n",
      "vectorized numpy loss: 2.3709406736983016, takes 0.001544952392578125 seconds.\n",
      "****************************************************************************************************\n",
      "Relative loss error of naive softmax is 2.3709404468536377\n",
      "Relative loss error of vectorized softmax is 2.268446639419608e-07\n",
      "Gradient error of naive softmax is 2.0511879407852547\n",
      "Gradient error of vectorized softmax is 2.0511879407852547\n"
     ]
    }
   ],
   "source": [
    "from ecbm4040.classifiers.softmax import softmax_loss_naive\n",
    "from ecbm4040.classifiers.softmax import softmax_loss_vectorized\n",
    "\n",
    "# generate a random SVM weight matrix of small numbers\n",
    "np.random.seed(2321)\n",
    "W = np.random.randn(3073, 10) * 0.0001 \n",
    "\n",
    "# ground truth of loss and gradient\n",
    "W_tf = tf.placeholder(tf.float32, shape=(3073,10))\n",
    "X = tf.placeholder(tf.float32, shape=(None, 3073))\n",
    "y = tf.placeholder(tf.int32, shape=(None,))\n",
    "reg = tf.constant(0.000005)\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits= tf.matmul(X, W_tf), labels=tf.one_hot(y,10))\n",
    "loss0 = tf.reduce_mean(cross_entropy) + reg*tf.reduce_sum(W_tf*W_tf)\n",
    "grad0 = tf.gradients(loss0, W_tf)\n",
    "out0 = (loss0, grad0)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    tic = time.time()\n",
    "    loss_gt, grad_gt = sess.run(out0, feed_dict={W_tf: W, X: X_dev, y: y_dev})\n",
    "    toc = time.time()\n",
    "\n",
    "# naive softmax in numpy\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive numpy loss: {}, takes {} seconds.'.format(loss_naive, toc-tic))\n",
    "\n",
    "# vectorized softmax in numpy\n",
    "tic = time.time()\n",
    "loss_vec, grad_vec = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized numpy loss: {}, takes {} seconds.'.format(loss_vec, toc-tic))\n",
    "\n",
    "# Verify your result here - use 'rel_err' for error evaluation.\n",
    "def rel_err(a,b):\n",
    "    return np.mean(abs(a-b))\n",
    "\n",
    "print('*'*100)\n",
    "print('Relative loss error of naive softmax is {}'.format(rel_err(loss_gt,loss_naive)))\n",
    "print('Relative loss error of vectorized softmax is {}'.format(rel_err(loss_gt,loss_vec)))\n",
    "print('Gradient error of naive softmax is {}'.format(rel_err(grad_gt,grad_naive)))\n",
    "print('Gradient error of vectorized softmax is {}'.format(rel_err(grad_gt,grad_vec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss output of tensorflow softmax is 2.3709404468536377\n"
     ]
    }
   ],
   "source": [
    "# softmax in tensorflow\n",
    "W_tf = tf.placeholder(tf.float32, shape=(3073,10))\n",
    "X = tf.placeholder(tf.float32, shape=(None, 3073))\n",
    "y = tf.placeholder(tf.int32, shape=(None,))\n",
    "reg = tf.constant(0.000005)\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# Understand how to use this function \"tf.nn.softmax_cross_entropy_with_logits\".\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits= tf.matmul(X, W_tf), labels=tf.one_hot(y,10))\n",
    "loss_tf = tf.reduce_mean(cross_entropy) + reg*tf.reduce_sum(W_tf*W_tf)\n",
    "grad_tf = tf.gradients(loss_tf, W_tf)\n",
    "out = (loss_tf, grad_tf)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    tic = time.time()\n",
    "    loss, grad = sess.run(out, feed_dict={W_tf: W, X: X_dev, y: y_dev})\n",
    "    toc = time.time()\n",
    "    \n",
    "print(\"loss output of tensorflow softmax is {}\".format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Train your classifiers\n",
    "\n",
    "Now you can start to train your classifiers. We are going to use gradient descent algorithm for training, which is differs from a usual SVM training process. \n",
    "\n",
    "<span style=\"color:red\"><strong>TODO</strong></span>: The original code is given in **./ecbm4040/classifiers/basic_classifier.py**. You need to finish **train** and **predict** functions in the class **BasicClassifier**. Later, you use its subclasses **LinearSVM** and **Softmax** to train the model seperately and verify your result.\n",
    "\n",
    "In the training section, you are asked to implement two optimization methods, (a)Stochastic gradient descent (SGD), and (b)SGD with momentum, (this one is optional). Pseudo code for SGD is shown below.\n",
    "\n",
    "* Stochastic gradient descent - SGD\n",
    "    ```\n",
    "    w = w - learning_rate * gradient \n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Linear SVM + SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ecbm4040.classifiers.basic_classifiers import LinearSVM\n",
    "\n",
    "# Linear SVM + SGD\n",
    "classifier = LinearSVM()\n",
    "reg = 1e-5 # regularization\n",
    "lr = 1e-7 # learning rate\n",
    "loss_hist_sgd = classifier.train(X=X_train, y=y_train, learning_rate=lr, reg=reg, num_iters=1500, optim='SGD', verbose=True)\n",
    "\n",
    "# Write the LinearSVM.predict function and evaluate the performance on both the\n",
    "# training set and validation set\n",
    "y_train_pred = classifier.predict(X_train)\n",
    "print('training accuracy: %f' % (np.mean(y_train == y_train_pred), ))\n",
    "y_val_pred = classifier.predict(X_val)\n",
    "print('validation accuracy: %f' % (np.mean(y_val == y_val_pred), ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SGD Error Plot (loss curve)\n",
    "plt.plot(loss_hist_sgd, label='SGD')\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Softmax + SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ecbm4040.classifiers.basic_classifiers import Softmax\n",
    "\n",
    "# Linear SVM + SGD\n",
    "classifier = Softmax()\n",
    "reg = 1e-5 # regularization\n",
    "lr = 1e-7 # learning rate\n",
    "loss_hist_sgd = classifier.train(X=X_train, y=y_train, learning_rate=lr, reg=reg, num_iters=1500, optim='SGD', verbose=True)\n",
    "\n",
    "# Write the LinearSVM.predict function and evaluate the performance on both the\n",
    "# training and validation set\n",
    "y_train_pred = classifier.predict(X_train)\n",
    "print('training accuracy: %f' % (np.mean(y_train == y_train_pred), ))\n",
    "y_val_pred = classifier.predict(X_val)\n",
    "print('validation accuracy: %f' % (np.mean(y_val == y_val_pred), ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SGD loss curve\n",
    "plt.plot(loss_hist_sgd, label='SGD')\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
